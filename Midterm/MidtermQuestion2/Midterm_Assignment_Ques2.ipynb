{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahesh\\Miniconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\mahesh\\Miniconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the date!\n",
      "Could not find the author!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#import requests\n",
    "\n",
    "# Ask user to enter URL\n",
    "url = \"http://www.nytimes.com/\"\n",
    "\n",
    "# Open txt document for output\n",
    "txt = open('ctp_output.txt', 'w')\n",
    "\n",
    "# Parse HTML of article, aka making soup\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url).read())\n",
    "\n",
    "# Write the article title to the file    \n",
    "title = soup.find(\"h1\")\n",
    "txt.write('\\n' + \"Title: \" + title.string + '\\n' + '\\n')\n",
    "\n",
    "# Write the article date to the file    \n",
    "try:\n",
    "    date = soup.find(\"span\", {'class':'dateline'}).text\n",
    "    txt.write(\"Date: \" + str(date) + '\\n' + '\\n')\n",
    "except:\n",
    "    print(\"Could not find the date!\")\n",
    "\n",
    "# Write the article author to the file    \n",
    "try:\n",
    "    byline=soup.find(\"p\", {'class':'byline-author'}).text\n",
    "    txt.write(\"Author: \" + str(byline) + '\\n' + '\\n')\n",
    "except:\n",
    "    print(\"Could not find the author!\")\n",
    "\n",
    "# Write the article location to the file    \n",
    "regex = '<span class=\"location\">(.+?)</span>'\n",
    "pattern = re.compile(regex)\n",
    "byline = re.findall(pattern,str(soup))\n",
    "txt.write(\"Location: \" + str(byline) + '\\n' + '\\n')\n",
    "\n",
    "# retrieve all of the paragraph tags\n",
    "with open('ctp_output.txt', 'w'):\n",
    "    for tag in soup.find_all('p'):\n",
    "        #txt.write(tag.text.encode('utf-8') + '\\n' + '\\n')\n",
    "\n",
    "# Close txt file with new content added\n",
    "        txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Next we use the get function in requests library to store data in a local variable called resp.\n",
    "#This gives us everything in the form of a continuous string. We need to change this string into json format. So we use the loads function of json library to do that and store the json data into a variable called data.\n",
    "import json, requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "archive_key = os.getenv('archive_key') \n",
    "\n",
    "#url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=new+york+times&page=2&sort=oldest&api-key=nyt_article_key'\n",
    "#resp = requests.get(url)\n",
    "#data = json.loads(resp.text)\n",
    "#pprint(data) \n",
    "#type(data)\n",
    "date=20141201\n",
    "page=1\n",
    "url = \"https://api.nytimes.com/svc/archive/v1/2016/1.json?begin_date=\" + str(date) + \"&end_date=\" + str(date) + \"&page=\" + str(page) + \"&api-key=\" + str(archive_key)\n",
    "response = requests.get(url).json()\n",
    "resp = requests.get(url)\n",
    "#ata = json.loads(resp.text)\n",
    "#print(data) \n",
    "with open(str(date) + 'archive' + str(page)+ '.json', 'w') as fp:\n",
    "    json.dump(response, fp)\n",
    "    page += 1\n",
    "    date += 1\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-945d3673e0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;31m#pprint(data[\"response\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"response\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"docs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for key in data:\n",
    "\n",
    "    print(key)\n",
    "    #pprint(data[\"response\"])\n",
    "    pprint(data[\"response\"][\"docs\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LATEST INTELLIGENCE; From Wahington--Population of South Carolina--The '\n",
      " \"President's Health, &c.\")\n",
      "\"PHILADELPHIA.; Odd Fellows' Parade--The Christiana Murder.\"\n",
      "'Matters in Michigan.'\n",
      "'The State Fair.; THIRD DAY.'\n",
      "'Funeral of James Fenimore Cooper.'\n",
      "'Lumber Trade of Michigan.'\n",
      "('LATEST INTELLIGENCE; Dreadful Steamboat Accident--Thirty-five Persons Killed '\n",
      " 'and Wounded. The Great Telegraph Case--Morse versus Bain. From '\n",
      " 'Washington--Extension of the Capitol-- The Texas Debt. Portland and Halifax '\n",
      " 'Railway. Items from South. The Canal Breaks. Navigation of the Ohio.')\n",
      "(\"MICHIGAN.; Railroad Conspiracy Trials--Seward's Speech-- Gen. Scott--Gen. \"\n",
      " 'Cass--Travel--Canada Road --Business at Detroit--Rapid Settlement on Lake '\n",
      " 'Superior.')\n",
      "'An Appeal to the American Public, in Behalf of the Very Rev. Father Mathew.'\n",
      "('NEW PUBLICATONS.; THE HISTORY OF THE RESTORATION OF MONARCHY IN FRANCE. By '\n",
      " 'ALPHONSE DE LAMARTINE. New-York: Harper & Brothers.')\n"
     ]
    }
   ],
   "source": [
    "for key in data[\"response\"][\"docs\"]:\n",
    "\n",
    "    pprint(key[\"headline\"][\"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LATEST INTELLIGENCE; From Wahington--Population of South Carolina--The '\n",
      " \"President's Health, &c.\")\n",
      "\"PHILADELPHIA.; Odd Fellows' Parade--The Christiana Murder.\"\n",
      "'Matters in Michigan.'\n",
      "'The State Fair.; THIRD DAY.'\n",
      "'Funeral of James Fenimore Cooper.'\n",
      "'Lumber Trade of Michigan.'\n",
      "('LATEST INTELLIGENCE; Dreadful Steamboat Accident--Thirty-five Persons Killed '\n",
      " 'and Wounded. The Great Telegraph Case--Morse versus Bain. From '\n",
      " 'Washington--Extension of the Capitol-- The Texas Debt. Portland and Halifax '\n",
      " 'Railway. Items from South. The Canal Breaks. Navigation of the Ohio.')\n",
      "(\"MICHIGAN.; Railroad Conspiracy Trials--Seward's Speech-- Gen. Scott--Gen. \"\n",
      " 'Cass--Travel--Canada Road --Business at Detroit--Rapid Settlement on Lake '\n",
      " 'Superior.')\n",
      "'An Appeal to the American Public, in Behalf of the Very Rev. Father Mathew.'\n",
      "('NEW PUBLICATONS.; THE HISTORY OF THE RESTORATION OF MONARCHY IN FRANCE. By '\n",
      " 'ALPHONSE DE LAMARTINE. New-York: Harper & Brothers.')\n"
     ]
    }
   ],
   "source": [
    "query = \"Presidential\"\n",
    "\n",
    "url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=&#8221; + query + â€œ&page=2&sort=oldest&api-key=d94bedd25e2f4424881ce91cee408eb6\"\n",
    "for key in data[\"response\"][\"docs\"]:\n",
    "    .\n",
    "\n",
    "    pprint(key[\"headline\"][\"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('json.txt','w') as myfile:\n",
    "    json.dump(data,myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nyt_article_search__key = os.getenv('auth_key')\n",
    "print(nyt_article_search__key)\n",
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=new+york+times&page=41&sort=newest&api-key=33be9c525b7d4220a3c52a788d9d68d6'\n",
    "response = requests.get(url).json()\n",
    "with open('response_1.json', 'w') as fp:\n",
    "    json.dump(response, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "api_key='33be9c525b7d4220a3c52a788d9d68d6'\n",
    "date = 20121201\n",
    "page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=\" + str(date) + \"&end_date=\" + str(date) + \"&page=\" + str(page) + \"&api-key=\" + str(api_key)\n",
    "response = requests.get(url).json()\n",
    "with open(str(date) + '_article_' + str(page)+ '.json', 'w') as fp:\n",
    "    json.dump(response, fp)\n",
    "    page += 1\n",
    "    date += 1\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path_to_json = 'C:\\\\Users\\\\mahesh\\\\Desktop\\\\Data analysis spring 2017\\\\Data_Analysis_Assignments\\\\wagh_rijuta_spring2017\\\\Midterm\\\\Articlejson'\n",
    "path_file= 'C:\\\\Users\\\\mahesh\\\\Desktop\\\\Data analysis spring 2017\\\\Data_Analysis_Assignments\\\\wagh_rijuta_spring2017\\\\Midterm\\\\ArticleResult'\n",
    "#path = '\\\\users\\\\kruts\\\\Midterm2Result'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print (json_files) \n",
    "\n",
    "def ensure_dir(path_file):\n",
    "    #directory = os.path.dirname(path_file)\n",
    "    #print(path_file)\n",
    "    if not os.path.exists(path_file):\n",
    "        os.makedirs(path_file)\n",
    "        #print(path_file)\n",
    "    #return directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "file = \"*.json\"\n",
    "\n",
    "from shutil import copy\n",
    "for js in glob.glob(os.path.join(path_to_json,file)):\n",
    "            json_file_read = json.loads(open(js).read())\n",
    "            for key,value in json_file_read.items():\n",
    "                #print(key)\n",
    "                if key == \"response\": # creation of folders\n",
    "                    \n",
    "                    meta = value.get('meta')\n",
    "                    #print(meta)\n",
    "                    docs = value.get('docs')\n",
    "                    for article in docs :\n",
    "                        sect_name = article.get('section_name')\n",
    "                        sect_name = str(sect_name).replace(\"/\",\" \")\n",
    "                        sect_name = str(sect_name).replace(\":\",\" \")\n",
    "                        #hits = meta.get('hits')\n",
    "                        path = os.path.join(path_file + '\\\\' + str(sect_name))\n",
    "                        #path = os.path.join(path + '/' + hits)\n",
    "                        #print(path)\n",
    "                        ensure_dir(path)\n",
    "                        copy(js,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collecting  data using Archive API\n",
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "api_key='33be9c525b7d4220a3c52a788d9d68d6'\n",
    "date = 20121201\n",
    "page = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://api.nytimes.com/svc/archive/v1/2016/1.json?begin_date=\" + str(date) + \"&end_date=\" + str(date) + \"&page=\" + str(page) + \"&api-key=\" + str(api_key)\n",
    "response = requests.get(url).json()\n",
    "with open(str(date) + '_archive_' + str(page)+ '.json', 'w') as fp:\n",
    "    json.dump(response, fp)\n",
    "    page += 1\n",
    "    date += 1\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "path_to_json = 'C:\\\\Users\\\\mahesh\\\\Desktop\\\\Data analysis spring 2017\\\\Data_Analysis_Assignments\\\\wagh_rijuta_spring2017\\\\Midterm\\\\ArchiveJson'\n",
    "path_file= 'C:\\\\Users\\\\mahesh\\\\Desktop\\\\Data analysis spring 2017\\\\Data_Analysis_Assignments\\\\wagh_rijuta_spring2017\\\\Midterm\\\\ArchiveResult'\n",
    "#path = '\\\\users\\\\kruts\\\\Midterm2Result'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print (json_files) \n",
    "\n",
    "def ensure_dir(path_file):\n",
    "    #directory = os.path.dirname(path_file)\n",
    "    #print(path_file)\n",
    "    if not os.path.exists(path_file):\n",
    "        os.makedirs(path_file)\n",
    "        #print(path_file)\n",
    "    #return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "file = \"*.json\"\n",
    "\n",
    "from shutil import copy\n",
    "for js in glob.glob(os.path.join(path_to_json,file)):\n",
    "            json_file_read = json.loads(open(js).read())\n",
    "            for key,value in json_file_read.items():\n",
    "                #print(key)\n",
    "                if key == \"response\": # creation of folders\n",
    "                    \n",
    "                    meta = value.get('meta')\n",
    "                    #print(meta)\n",
    "                    docs = value.get('docs')\n",
    "                    for article in docs :\n",
    "                        sect_name = article.get('section_name')\n",
    "                        sect_name = str(sect_name).replace(\"/\",\" \")\n",
    "                        sect_name = str(sect_name).replace(\":\",\" \")\n",
    "                        #hits = meta.get('hits')\n",
    "                        path = os.path.join(path_file + '\\\\' + str(sect_name))\n",
    "                        #path = os.path.join(path + '/' + hits)\n",
    "                        #print(path)\n",
    "                        ensure_dir(path)\n",
    "                        copy(js,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-87b30af28739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0martdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;31m#                         print(artdata.kesys)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mahesh\\Miniconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mahesh\\Miniconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mahesh\\Miniconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mahesh\\Miniconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "c=0\n",
    "b=0\n",
    "\n",
    "jpath = 'C:\\\\Users\\\\mahesh\\\\Desktop\\\\Data analysis spring 2017\\\\Data_Analysis_Assignments\\\\wagh_rijuta_spring2017\\\\Midterm\\\\ArchiveResult'\n",
    "        \n",
    "for root, dirs, files in os.walk(jpath):\n",
    "        \n",
    "            for f in files:\n",
    "                if f.endswith('.json'):\n",
    "                    with open(os.path.join(root,f), 'r', encoding='utf-8', errors=\"ignore\") as json_file:      \n",
    "                        artdata=json.load(json_file)\n",
    "#                         print(artdata.kesys)\n",
    "                        \n",
    "                        \n",
    "                       \n",
    "                        if 'pub_date' in artdata.keys():\n",
    "                            match= re.search(r'\\d{4}-\\d{2}-\\d{2}', artdata['pub_date'])\n",
    "\n",
    "                            date = datetime.datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "\n",
    "                            for i in range(0,len(artdata['keywords'])):\n",
    "                                    if 'glocation' in artdata['keywords'][i]['name']:\n",
    "                                        if 'boston' in artdata['keywords'][i]['value'].lower():\n",
    "                                                try:\n",
    "                                                    if 'terror' in artdata['lead_paragraph'].lower():\n",
    "                                                        c+=1\n",
    "                                                    if 'shooting' in artdata['lead_paragraph'].lower():\n",
    "                                                        c+=1\n",
    "                                                    if 'murder' in artdata['lead_paragraph'].lower():\n",
    "                                                        c+=1\n",
    "                                                except AttributeError:\n",
    "                                                    pass\n",
    "                                                \n",
    "                                        if 'california' in artdata['keywords'][i]['value'].lower():\n",
    "                                                try:\n",
    "                                                    if 'terror' in artdata['lead_paragraph'].lower():\n",
    "                                                        b+=1\n",
    "                                                    if 'shooting' in artdata['lead_paragraph'].lower():\n",
    "                                                        b+=1\n",
    "                                                    if 'murder' in artdata['lead_paragraph'].lower():\n",
    "                                                        b+=1\n",
    "                                                except AttributeError:\n",
    "                                                    pass\n",
    "                                                    \n",
    "\n",
    "print( 'Number of Shootings,crime and murder in boston is' ,+c)\n",
    "print( 'Number of Shootings,crime and murder in california is' ,+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
